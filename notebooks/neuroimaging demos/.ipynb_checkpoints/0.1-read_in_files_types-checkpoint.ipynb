{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Neuroimaging Python Reading in of Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import nibabel as nb\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "from nibabel.affines import apply_affine\n",
    "\n",
    "# import basic plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-df594002b1dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlut_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'multilevel_lookup_table.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlut_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlut_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlut_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_results' is not defined"
     ]
    }
   ],
   "source": [
    "lut_filepath = os.path.join(output_results, 'multilevel_lookup_table.txt')\n",
    "\n",
    "lut_df = pd.read_csv(lut_filepath, delimiter=\"\\t\", index_col=0)\n",
    "colnames = lut_df.columns\n",
    "\n",
    "# map nan elements to some empty str\n",
    "lut_df.isna\n",
    "\n",
    "# for col in colnames:\n",
    "#     print(col, np.unique(lut_df[col]))\n",
    "# display(lut_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the type of atlases\n",
    "atlas_list = lut_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MRI Cloud Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datafiles from mricloud: \n",
      "['priority.txt',\n",
      " 'identity.txt',\n",
      " 'output',\n",
      " 'config.txt',\n",
      " 'local_address.txt',\n",
      " 'atlas_list.txt']\n",
      "['matrix.txt',\n",
      " 'mni.imgsize',\n",
      " 'umf001_t1.img',\n",
      " 'umf001_t1_MNI.hdr',\n",
      " 'multilevel_lookup_table.txt',\n",
      " 'umf001_t1_286Labels_MNI.img',\n",
      " 'umf001_t1_286Labels.nii',\n",
      " 'umf001_t1.imgsize',\n",
      " 'umf001_t1_286Labels_corrected_stats.txt',\n",
      " 'umf001_t1.hdr',\n",
      " 'umf001_t1_7Labels_MNI.img',\n",
      " 'matrix_air.txt',\n",
      " 'umf001_t1_7Labels.hdr',\n",
      " 'umf001_t1_7Labels_MNI.hdr',\n",
      " 'umf001_t1_7Labels.img',\n",
      " 'umf001_t1_286Labels.hdr',\n",
      " 'umf001_t1_286Labels.img',\n",
      " 'umf001_t1_286Labels_MNI_stats.txt',\n",
      " 'umf001_t1_MNI.img',\n",
      " 'umf001_t1_286Labels_MNI.hdr']\n"
     ]
    }
   ],
   "source": [
    "patid = \"umf001\"\n",
    "\n",
    "datadir = \"/Users/adam2392/Downloads/neuroimage_data/la02_mricloud_result/\"\n",
    "datadir = f\"/home/adam2392/Documents/Dropbox/phd_research/data/neuroimaging_results/mricloud_results/{patid}\"\n",
    "output_results = os.path.join(datadir, 'output')\n",
    "\n",
    "check_files = os.listdir(datadir)\n",
    "datafiles = os.listdir(output_results)\n",
    "\n",
    "print(\"Datafiles from mricloud: \")\n",
    "pprint(check_files)\n",
    "pprint(datafiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
      " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
      " 154. 155. 156. 157. 158. 159. 160. 162. 163. 164. 165. 166. 167. 168.\n",
      " 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n",
      " 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196.\n",
      " 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210.\n",
      " 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n",
      " 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n",
      " 239. 240. 241. 242. 243. 246. 247. 248. 249. 250. 251. 252. 253. 254.\n",
      " 255. 256. 257. 258. 259. 260. 261. 263. 264. 265. 266. 267. 268. 269.\n",
      " 270. 271. 272. 273. 274. 275. 276. 277. 278. 279. 280. 281. 283. 285.\n",
      " 286.]\n"
     ]
    }
   ],
   "source": [
    "# read the img/hdr file\n",
    "# read in header\n",
    "hdr = nb.load(os.path.join(output_results, f'{patid}_t1_286Labels.hdr'))\n",
    "\n",
    "# img\n",
    "img = nb.load(os.path.join(output_results, f'{patid}_t1_286Labels.img'))\n",
    "\n",
    "img.hdr = hdr\n",
    "\n",
    "# convert to nifti\n",
    "nb.save(img, os.path.join(output_results, f'{patid}_t1_286Labels.nii'))\n",
    "\n",
    "# print(hdr)\n",
    "imgdata = img.get_fdata()\n",
    "\n",
    "print(np.unique(imgdata))\n",
    "# print(imgdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "aff_mat = img.affine\n",
    "\n",
    "# inverse affine\n",
    "anat2vox = np.linalg.inv(aff_mat)#.dot(aff_mat)\n",
    "\n",
    "print(anat2vox.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.      1.      0.46875 0.46875 0.      0.      0.      0.     ]\n"
     ]
    }
   ],
   "source": [
    "print(img.header['pixdim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['btm1', 'btm2', 'btm3', 'btm4', 'btm5', 'btm6', 'btp1', 'btp2', 'btp3', 'btp4', 'btp5', 'btp6', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10', 'c11', 'c12', 'c13', 'c14', 'c15', 'c16', 'c17', 'c18', 'c19', 'c20', 'c21', 'c22', 'c23', 'c24', 'c25', 'c26', 'c27', 'c28', 'c29', 'c30', 'c31', 'c32', 'c33', 'c34', 'c35', 'c36', 'c37', 'c38', 'c39', 'c40', 'c41', 'c42', 'c43', 'c44', 'c45', 'c46', 'c47', 'c48', 'c49', 'c50', 'c51', 'c52', 'c53', 'c54', 'c55', 'c56', 'c57', 'c58', 'c59', 'c60', 'c61', 'c62', 'c63', 'c64']\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'anatomy', 'eleclabels', 'elecmatrix'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MatReader():\n",
    "    '''\n",
    "    Object to read mat files into a nested dictionary if need be.\n",
    "    Helps keep strucutre from matlab similar to what is used in python.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, filename=None):\n",
    "        self.filename = filename\n",
    "\n",
    "    def loadmat(self, filename):\n",
    "        '''\n",
    "        this function should be called instead of direct spio.loadmat\n",
    "        as it cures the problem of not properly recovering python dictionaries\n",
    "        from mat files. It calls the function check keys to cure all entries\n",
    "        which are still mat-objects\n",
    "        '''\n",
    "        data = scipy.io.loadmat(\n",
    "            filename,\n",
    "            struct_as_record=False,\n",
    "            squeeze_me=True)\n",
    "        return self._check_keys(data)\n",
    "\n",
    "    def _check_keys(self, dict):\n",
    "        '''\n",
    "        checks if entries in dictionary are mat-objects. If yes\n",
    "        todict is called to change them to nested dictionaries\n",
    "        '''\n",
    "        for key in dict:\n",
    "            if isinstance(dict[key], scipy.io.matlab.mio5_params.mat_struct):\n",
    "                dict[key] = self._todict(dict[key])\n",
    "        return dict\n",
    "\n",
    "    def _todict(self, matobj):\n",
    "        '''\n",
    "        A recursive function which constructs from matobjects nested dictionaries\n",
    "        '''\n",
    "        dict = {}\n",
    "        for strg in matobj._fieldnames:\n",
    "            elem = matobj.__dict__[strg]\n",
    "            if isinstance(elem, scipy.io.matlab.mio5_params.mat_struct):\n",
    "                dict[strg] = self._todict(elem)\n",
    "            elif isinstance(elem, np.ndarray):\n",
    "                dict[strg] = self._tolist(elem)\n",
    "            else:\n",
    "                dict[strg] = elem\n",
    "        return dict\n",
    "\n",
    "    def _tolist(self, ndarray):\n",
    "        '''\n",
    "        A recursive function which constructs lists from cellarrays\n",
    "        (which are loaded as numpy ndarrays), recursing into the elements\n",
    "        if they contain matobjects.\n",
    "        '''\n",
    "        elem_list = []\n",
    "        for sub_elem in ndarray:\n",
    "            if isinstance(sub_elem, scipy.io.matlab.mio5_params.mat_struct):\n",
    "                elem_list.append(self._todict(sub_elem))\n",
    "            elif isinstance(sub_elem, np.ndarray):\n",
    "                elem_list.append(self._tolist(sub_elem))\n",
    "            else:\n",
    "                elem_list.append(sub_elem)\n",
    "        return elem_list\n",
    "\n",
    "    def convertMatToJSON(self, matData, fileName):\n",
    "        jsonData = {}\n",
    "\n",
    "        for key in matData.keys():\n",
    "            if (type(matData[key])) is np.ndarray:\n",
    "                serializedData = pickle.dumps(\n",
    "                    matData[key], protocol=0)  # protocol 0 is printable ASCII\n",
    "                jsonData[key] = serializedData\n",
    "            else:\n",
    "                jsonData[key] = matData[key]\n",
    "\n",
    "        with contextlib.closing(bz2.BZ2File(fileName, 'wb')) as f:\n",
    "            json.dump(jsonData, f)\n",
    "elecsdict = MatReader().loadmat(elecfile)\n",
    "elecmatrix = elecsdict['elecmatrix']\n",
    "eleclabels = elecsdict['eleclabels']\n",
    "elecanat = elecsdict['anatomy']\n",
    "\n",
    "eleclabels = [x[0] for x in eleclabels]\n",
    "elecanat = [x[3] for x in elecanat]\n",
    "\n",
    "# print(elecanat)\n",
    "# print(elecmatrix)\n",
    "print(eleclabels)\n",
    "print(elecsdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'anatomy', 'eleclabels', 'elecmatrix'])\n"
     ]
    }
   ],
   "source": [
    "# read in elecs file\n",
    "elecfile = f\"/home/adam2392/hdd/data/neuroimaging/freesurfer_output/{patid}/elecs/{patid}_elecs_all.mat\"\n",
    "\n",
    "elecsdict = scipy.io.loadmat(elecfile, )\n",
    "elecmatrix = elecsdict['elecmatrix']\n",
    "eleclabels = elecsdict['eleclabels']\n",
    "\n",
    "# print(elecmatrix)\n",
    "# print(eleclabels)\n",
    "print(elecsdict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('R', 'A', 'S')\n",
      "('L', 'A', 'S')\n"
     ]
    }
   ],
   "source": [
    "canon = nb.as_closest_canonical(img)\n",
    "print(nb.aff2axcodes(canon.affine))\n",
    "print(nb.aff2axcodes(img.affine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 41., 186., 166.]), array([ 34., 196., 162.]), array([ 27., 207., 159.]), array([ 16., 219., 160.]), array([  7., 233., 162.]), array([  1., 245., 171.]), array([ 32., 131., 169.]), array([ 26., 149., 166.]), array([ 20., 164., 166.]), array([ 12., 183., 166.]), array([  6., 199., 166.]), array([ -0., 217., 172.]), array([ 52., 408., 289.]), array([ 34., 390., 302.]), array([ 25., 372., 310.]), array([ 23., 352., 320.]), array([ 21., 330., 328.]), array([ 19., 308., 338.]), array([ 18., 288., 346.]), array([ 19., 266., 354.]), array([ 35., 403., 270.]), array([ 22., 382., 284.]), array([ 18., 363., 294.]), array([ 15., 341., 303.]), array([ 13., 322., 311.]), array([ 11., 300., 320.]), array([ 10., 279., 329.]), array([ 10., 260., 337.]), array([ 24., 394., 254.]), array([ 18., 375., 261.]), array([ 14., 353., 271.]), array([ 11., 333., 281.]), array([  8., 312., 293.]), array([  6., 293., 300.]), array([  4., 271., 310.]), array([  3., 252., 318.]), array([ 20., 377., 238.]), array([ 16., 365., 243.]), array([ 11., 344., 252.]), array([  9., 327., 263.]), array([  6., 302., 272.]), array([  4., 284., 281.]), array([  2., 265., 291.]), array([ -0., 244., 302.]), array([  8., 315., 214.]), array([  4., 300., 231.]), array([  3., 288., 247.]), array([  1., 272., 260.]), array([ -1., 254., 274.]), array([ -2., 235., 289.]), array([ -3., 221., 302.]), array([ -1., 200., 315.]), array([  8., 307., 199.]), array([  3., 288., 219.]), array([  0., 272., 231.]), array([ -2., 256., 244.]), array([ -3., 242., 257.]), array([ -5., 221., 274.]), array([ -5., 207., 286.]), array([ -3., 188., 302.]), array([  7., 293., 187.]), array([  2., 275., 200.]), array([ -1., 260., 215.]), array([ -3., 245., 228.]), array([ -6., 225., 245.]), array([ -6., 210., 255.]), array([ -5., 194., 271.]), array([ -5., 176., 284.]), array([  7., 277., 171.]), array([  0., 259., 183.]), array([ -3., 243., 198.]), array([ -5., 229., 210.]), array([ -6., 212., 227.]), array([ -6., 195., 241.]), array([ -6., 177., 254.]), array([ -4., 160., 266.])]\n"
     ]
    }
   ],
   "source": [
    "elecvox = [np.round(apply_affine(anat2vox, x)) for x in elecmatrix]\n",
    "\n",
    "print(elecvox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Coordinate System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old code:  ('L', 'I', 'P')\n",
      "New code:  ('L', 'A', 'S')\n",
      "('L', 'I', 'P')\n",
      "(117, 512, 512, 1)\n",
      "(117, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "orig_t1 = f\"/home/adam2392/hdd/data/neuroimaging/freesurfer_output/{patid}/mri/T1.nii.gz\"\n",
    "orig_img = nb.load(orig_t1)\n",
    "\n",
    "# get original orientation\n",
    "orig_code = nb.aff2axcodes(orig_img.affine)\n",
    "\n",
    "# end orientation\n",
    "end_code = nb.aff2axcodes(img.affine)\n",
    "\n",
    "ornt_orig = nb.orientations.axcodes2ornt(orig_code)\n",
    "ornt_end = nb.orientations.axcodes2ornt(end_code)\n",
    "\n",
    "# transform start to end\n",
    "ornt_start_to_end = nb.orientations.ornt_transform(ornt_orig, ornt_end)\n",
    "\n",
    "# affine start to end\n",
    "aff_start_end = nb.orientations.inv_ornt_aff(ornt_start_to_end, shape=img.shape)\n",
    "\n",
    "new_img_data = nb.orientations.apply_orientation(imgdata, ornt_start_to_end)\n",
    "\n",
    "# new affine\n",
    "newaff = aff_start_end.dot(img.affine)\n",
    "# newaff = img.affine.dot(aff_start_end)\n",
    "newcode = nb.aff2axcodes(newaff)\n",
    "\n",
    "print(\"Old code: \", orig_code)\n",
    "print(\"New code: \", end_code)\n",
    "print(nb.aff2axcodes(newaff))\n",
    "# print(newcode)\n",
    "\n",
    "print(imgdata.shape)\n",
    "print(new_img_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Voxels into XYZ Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(i, j, k):\n",
    "    \"\"\" Return X, Y, Z coordinates for i, j, k \"\"\"\n",
    "    return M.dot([i, j, k]) + abc\n",
    "from nibabel.affines import apply_affine\n",
    "\n",
    "# for i in range()\n",
    "# apply_affine(img.affine, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgtypes = [\n",
    "    '',\n",
    "    '_7Labels',\n",
    "    '_7Labels_MNI',\n",
    "    '_286Labels',\n",
    "    '_286Labels_MNI',\n",
    "    '_MNI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "THIS FILE HAS IMAGE SIZE INFORMATION FOR ANALYZE IMG FILE\n",
      "/work/00303/tg455757/stampede2/M1_101214/atlassets/MNI/JHU_MNI_T1.img\n",
      "********************************************\n",
      "1) Nx, Ny, Nz : IMGSIZE\n",
      "181\t217\t181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imgsizes = [\n",
    "    'mni',\n",
    "#     'la02_t1_image'\n",
    "]\n",
    "\n",
    "for _imgsize in imgsizes:\n",
    "    # img-size\n",
    "    with open(os.path.join(output_results, '{}.imgsize'.format(_imgsize)), 'r') as f:\n",
    "        imgsize = f.read()\n",
    "    print(imgsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ('L', 'A', 'S')\n",
      "_7Labels ('L', 'A', 'S')\n",
      "_7Labels_MNI ('L', 'A', 'S')\n",
      "_286Labels ('L', 'A', 'S')\n",
      "_286Labels_MNI ('L', 'A', 'S')\n",
      "_MNI ('L', 'A', 'S')\n"
     ]
    }
   ],
   "source": [
    "# read in header\n",
    "hdr = nb.load(os.path.join(output_results, 'la02_t1_image_7Labels.hdr'))\n",
    "\n",
    "# img-size\n",
    "with open(os.path.join(output_results, 'la02_t1_image.imgsize'), 'r') as f:\n",
    "    imgsize = f.read()\n",
    "# print(imgsize)\n",
    "\n",
    "# img\n",
    "img = nb.load(os.path.join(output_results, 'la02_t1_image_7Labels.img'))\n",
    "orientation = nb.aff2axcodes(img.affine)\n",
    "\n",
    "# read hdr, img in loop\n",
    "for imgtype in imgtypes:\n",
    "    # read in header\n",
    "    hdr = nb.load(os.path.join(output_results, 'la02_t1_image{}.hdr'.format(imgtype)))\n",
    "\n",
    "    # img\n",
    "    img = nb.load(os.path.join(output_results, 'la02_t1_image{}.img'.format(imgtype)))\n",
    "    orientation = nb.aff2axcodes(img.affine)\n",
    "\n",
    "    print(imgtype, orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nibabel.spm2analyze.Spm2AnalyzeImage'>\n",
      "data shape (181, 217, 181, 1)\n",
      "affine: \n",
      "[[  -1.    0.    0.   90.]\n",
      " [   0.    1.    0. -108.]\n",
      " [   0.    0.    1.  -90.]\n",
      " [   0.    0.    0.    1.]]\n",
      "metadata:\n",
      "<class 'nibabel.spm2analyze.Spm2AnalyzeHeader'> object, endian='<'\n",
      "sizeof_hdr     : 348\n",
      "data_type      : b'16 bits'\n",
      "db_name        : b'DICOM->Analyze7.5'\n",
      "extents        : 16384\n",
      "session_error  : 0\n",
      "regular        : b'r'\n",
      "hkey_un0       : b''\n",
      "dim            : [  4 181 217 181   1   0   0   0]\n",
      "vox_units      : b'mm'\n",
      "cal_units      : b'  '\n",
      "unused1        : 0\n",
      "datatype       : float32\n",
      "bitpix         : 32\n",
      "dim_un0        : 0\n",
      "pixdim         : [0. 1. 1. 1. 0. 0. 0. 0.]\n",
      "vox_offset     : 0.0\n",
      "scl_slope      : nan\n",
      "scl_inter      : 0.0\n",
      "funused3       : 0.0\n",
      "cal_max        : 0.0\n",
      "cal_min        : 0.0\n",
      "compressed     : 0\n",
      "verified       : 0\n",
      "glmax          : 255\n",
      "glmin          : 0\n",
      "descrip        : b'Dcm2Analyze 2.0, Author: Hangyi Jiang'\n",
      "aux_file       : b'see: *.dat/*.txt files'\n",
      "orient         : b''\n",
      "origin         : [0 0 0 0 0]\n",
      "generated      : b''\n",
      "scannum        : b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00T\\xe3e'\n",
      "patient_id     : b'\\xbf\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00T'\n",
      "exp_date       : b'\\xe3\\xe5B\\x00\\x00\\x00\\x80\\r\\xad`'\n",
      "exp_time       : b'>M.\\xba\\xbf\\x9d\\x8e\\xb0B'\n",
      "hist_un0       : b''\n",
      "views          : 1063185321\n",
      "vols_added     : 1052485024\n",
      "start_field    : -1022572618\n",
      "field_skip     : 0\n",
      "omax           : 0\n",
      "omin           : 0\n",
      "smax           : 0\n",
      "smin           : 0\n"
     ]
    }
   ],
   "source": [
    "print(hdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This contains structure look-up table in a text format.\n",
    "lut_file = 'multilevel_lookup_table.txt',\n",
    "matairfile = 'matrix_air.txt'\n",
    "matfile = 'matrix.txt',\n",
    "corrected_stats_file = 'la02_t1_image_286Labels_corrected_stats.txt',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freesurfer Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = 'la03'\n",
    "\n",
    "# define freesurfer outputdir\n",
    "freesurfer_dir = \"\"\n",
    "\n",
    "# define the subject directory\n",
    "subj_dir = os.path.join(\"\", patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define volume file\n",
    "desikan_file = os.path.join(subj_dir, 'aparc+aseg.mgz')\n",
    "destrieux_file = os.path.join(subj_dir, 'aparc.a2009s+aseg.mgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in volume files\n",
    "desikan_vol = nb.load(desikan_file)\n",
    "destrieux_vol = nb.load(destrieux_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eztrack",
   "language": "python",
   "name": "eztrack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
